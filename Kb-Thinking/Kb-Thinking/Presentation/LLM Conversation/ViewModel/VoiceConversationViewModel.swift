//
//  VoiceConversationViewModel.swift
//  Kb-Thinking
//
//  Created by jeonguk29 on 8/8/25.
//

import Foundation
import SwiftUI
import Combine

@MainActor
final class VoiceConversationViewModel: ObservableObject {

    // MARK: - Published
    @Published var transcribedText: String = ""
    @Published var llmResponse: String = ""
    @Published var voiceState: VoiceState = .idle

    /// í™”ë©´(Presentation)ìœ¼ë¡œ ë¼ìš°íŒ… ì‹ í˜¸ ì „ë‹¬
    var onRoute: ((AppRoute?) -> Void)?

    // MARK: - Dependencies
    private let recognizer = SpeechRecognizerManager()
    private let synthesizer = SpeechSynthesizerManager()
    private let sendLLMMessageUseCase: SendLLMMessageUseCase

    // MARK: - Lifecycle
    init(sendLLMMessageUseCase: SendLLMMessageUseCase) {
        self.sendLLMMessageUseCase = sendLLMMessageUseCase

        recognizer.requestAuthorization { status in
            print("ê¶Œí•œ ìƒíƒœ:", status)
        }

        // ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ë¥¼ UI ë°”ì¸ë”©
        recognizer.$recognizedText
            .receive(on: DispatchQueue.main)
            .assign(to: &$transcribedText)
    }

    // MARK: - Public API (Viewì—ì„œ í˜¸ì¶œ)

    /// ë…¹ìŒ ì‹œì‘
    func startListening() {
        voiceState = .userSpeaking
        transcribedText = ""
        do {
            try recognizer.startRecording()
        } catch {
            print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:", error.localizedDescription)
        }
    }

    /// (ì‹¤í†µì‹ ) ë…¹ìŒ ì¢…ë£Œ í›„ LLMì— ì „ì†¡
    func stopListeningAndAskLLM() async {
        recognizer.stopRecording()
        let input = transcribedText

        voiceState = .aiSpeaking
        do {
            let entity = try await sendLLMMessageUseCase.execute(requestText: input)
            await handle(entity: entity)
        } catch {
            await handle(error: error)
        }
    }

    /// (ëª©ì—…) ë…¹ìŒ ì¢…ë£Œ í›„ ëª©ì—… ì‘ë‹µ ì²˜ë¦¬
    func stopListeningAndRespondWithMock(index: Int = 3) async {
        recognizer.stopRecording()
        voiceState = .aiSpeaking

        // ì¸ë±ìŠ¤ ì•ˆì „ ê°€ë“œ
        let list = LLMMessageEntity.mockList
        let mock = list[index]
        await handle(entity: mock)
    }

    /// í•©ì„± ì¤‘ë‹¨/ì´ˆê¸°í™”
    func stopSpeaking() {
        synthesizer.stop()
        voiceState = .idle
    }

    /// ì·¨ì†Œ(ë…¹ìŒ/í•©ì„± ì¢…ë£Œ + í…ìŠ¤íŠ¸ ì´ˆê¸°í™”)
    func cancel() {
        recognizer.stopRecording()
        synthesizer.stop()
        transcribedText = ""
        llmResponse = ""
        voiceState = .idle
    }

    /// Viewì—ì„œ â€œì „ì†¡â€ ë²„íŠ¼ì— ë§¤í•‘í•  í•¨ìˆ˜
    /// - ë°ëª¨: ëª©ì—…/ì‹¤í†µì‹  ì „í™˜ì€ ì—¬ê¸°ì„œ ìŠ¤ìœ„ì¹˜
    func sendToLLM(useMock: Bool = false) async {
        if useMock {
            await stopListeningAndRespondWithMock()
        } else {
            await stopListeningAndAskLLM()
        }
    }

    // MARK: - Private Common Handlers

    /// ì„±ê³µ ê³µí†µ ì²˜ë¦¬: í…ìŠ¤íŠ¸/í•©ì„±/ë¼ìš°íŒ… í•œ ë²ˆì—
    private func handle(entity: LLMMessageEntity) async {
        // UI ì—…ë°ì´íŠ¸
        llmResponse = entity.responseText
        transcribedText = entity.responseText

        // ìŒì„± í•©ì„±
        synthesizer.speak(text: entity.responseText)

        // ë¼ìš°íŒ… í•„ìš” ì‹œ ì•Œë¦¼ (Domain Route â†’ AppRoute)
        if let domain = entity.route,
           let app = RouteAdapter.map(domain) {
            // ì‹œíŠ¸ ë‹«í˜ ì• ë‹ˆì™€ ì¶©ëŒ ë°©ì§€ ì•½ê°„ì˜ ì—¬ìœ (í•„ìš” ì‹œ ì œê±°/ì¡°ì •)
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.15) { [weak self] in
                self?.onRoute?(app)
            }
        }

        voiceState = .idle
    }

    /// ì—ëŸ¬ ê³µí†µ ì²˜ë¦¬: ë©”ì‹œì§€ í‘œì‹œ + ìƒíƒœ ì •ë¦¬
    private func handle(error: Error) async {
        transcribedText = "âŒ ì‘ë‹µ ì‹¤íŒ¨: \(error.localizedDescription)"
        voiceState = .idle
    }
}
