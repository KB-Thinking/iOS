//
//  VoiceConversationViewModel.swift
//  Kb-Thinking
//
//  Created by jeonguk29 on 8/8/25.
//

import Foundation
import SwiftUI
import Combine

@MainActor
final class VoiceConversationViewModel: ObservableObject {
    
    // MARK: - Published Properties
    
    @Published var transcribedText: String = ""
    @Published var voiceState: VoiceState = .idle
    
    // MARK: - Dependencies
    
    private let recognizer = SpeechRecognizerManager()
    private let synthesizer = SpeechSynthesizerManager()
    private let sendLLMMessageUseCase: SendLLMMessageUseCase
    
    private var cancellables = Set<AnyCancellable>()
    
    init(sendLLMMessageUseCase: SendLLMMessageUseCase) {
        self.sendLLMMessageUseCase = sendLLMMessageUseCase
        
        recognizer.requestAuthorization { status in
            print("ê¶Œí•œ ìƒíƒœ: \(status)")
        }

        // Combine ë°”ì¸ë”©
        recognizer.$recognizedText
            .receive(on: DispatchQueue.main)
            .assign(to: &$transcribedText)
    }
    
    // MARK: - Methods
    
    func startListening() {
        voiceState = .userSpeaking
        transcribedText = ""

        do {
            try recognizer.startRecording()
        } catch {
            print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:", error.localizedDescription)
        }
    }
    
    func stopListeningAndAskLLM() async {
        recognizer.stopRecording()
        let input = transcribedText
        
        do {
            voiceState = .aiSpeaking
            let entity = try await sendLLMMessageUseCase.execute(requestText: input)
            transcribedText = entity.responseText
            synthesizer.speak(text: entity.responseText)
        } catch {
            transcribedText = "âŒ ì‘ë‹µ ì‹¤íŒ¨: \(error.localizedDescription)"
        }
        
        voiceState = .idle
    }
    
    func stopSpeaking() {
        synthesizer.stop()
        voiceState = .idle
    }
    
    // MARK: - ëª©ì—… ì‘ë‹µ
    func stopListeningAndRespondWithMock() async {
        recognizer.stopRecording()
        voiceState = .aiSpeaking
        
        // ëª©ì—… ì‘ë‹µ ì‚¬ìš©
        let mock = LLMMessageEntity.mockList.randomElement()
        let reply = mock?.responseText ?? "ì£„ì†¡í•´ìš”, ì‘ë‹µì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”."
        
        transcribedText = reply
        synthesizer.speak(text: reply)
        
        voiceState = .idle
    }
}
