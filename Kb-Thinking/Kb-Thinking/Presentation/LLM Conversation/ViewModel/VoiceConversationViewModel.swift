//
//  VoiceConversationViewModel.swift
//  Kb-Thinking
//
//  Created by jeonguk29 on 8/8/25.
//

import Foundation
import SwiftUI
import Combine

@MainActor
final class VoiceConversationViewModel: ObservableObject {

    // MARK: - Published
    @Published var transcribedText: String = ""
    @Published var llmResponse: String = ""
    @Published var voiceState: VoiceState = .idle
    @Published var isListening: Bool = false
    @Published var isProcessing: Bool = false
    @Published var isAuthorized: Bool = false

    /// í™”ë©´(Presentation)ìœ¼ë¡œ ë¼ìš°íŒ… ì‹ í˜¸ ì „ë‹¬
    var onRoute: ((AppRoute?) -> Void)?

    // MARK: - Dependencies
    private let recognizer = SpeechRecognizerManager()
    private let synthesizer: SpeechSynthesizerManager
    private let sendLLMMessageUseCase: SendLLMMessageUseCase

    // MARK: - State
    private var cancellables = Set<AnyCancellable>()

    // MARK: - Lifecycle
    init(
        sendLLMMessageUseCase: SendLLMMessageUseCase,
        synthesizer: SpeechSynthesizerManager? = nil,
        isPreview: Bool = false
    ) {
        self.sendLLMMessageUseCase = sendLLMMessageUseCase
        self.synthesizer = synthesizer ?? VoiceConversationViewModel.makeDefaultSynthesizer()
        setupBindings()
        
        if !isPreview {
            requestPermissions()
        } else {
            isAuthorized = true
        }
    }

    // MARK: - Factory
    private static func makeDefaultSynthesizer() -> SpeechSynthesizerManager {
        // 1) ëŸ°íƒ€ì„ í™˜ê²½ë³€ìˆ˜ (ì‹œë®¬ë ˆì´í„°/ê°œë°œìš©)
        if let key = ProcessInfo.processInfo.environment["OPENAI_API_KEY"], !key.isEmpty {
            let service = OpenAITTSAPIService(apiKey: key)
            return SpeechSynthesizerManager(ttsEngine: .openAI, openAIService: service)
        }
        // 2) Info.plist (ë°°í¬/ë””ë°”ì´ìŠ¤ìš©)
        if let key = Bundle.main.object(forInfoDictionaryKey: "OPENAI_API_KEY") as? String, !key.isEmpty {
            let service = OpenAITTSAPIService(apiKey: key)
            return SpeechSynthesizerManager(ttsEngine: .openAI, openAIService: service)
        }
        // 3) ê¸°ë³¸: ì‹œìŠ¤í…œ TTS
        return SpeechSynthesizerManager(ttsEngine: .system)
    }

    // MARK: - Setup
    private func setupBindings() {
        // ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ë¥¼ UI ë°”ì¸ë”©
        recognizer.$recognizedText
            .receive(on: DispatchQueue.main)
            .assign(to: &$transcribedText)
        
        // ìŒì„± ì¸ì‹ ìƒíƒœ ë°”ì¸ë”©
        recognizer.$isRecording
            .receive(on: DispatchQueue.main)
            .assign(to: &$isListening)
        
        // ê¶Œí•œ ìƒíƒœ ë°”ì¸ë”©
        recognizer.$isAuthorized
            .receive(on: DispatchQueue.main)
            .assign(to: &$isAuthorized)
        
        // ìŒì„± í•©ì„± ì™„ë£Œ ê°ì§€
        synthesizer.$isSpeaking
            .receive(on: DispatchQueue.main)
            .sink { [weak self] isSpeaking in
                if !isSpeaking && self?.voiceState == .aiSpeaking {
                    // AI ì‘ë‹µ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ idle ìƒíƒœë¡œ ì „í™˜
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        self?.voiceState = .idle
                    }
                }
            }
            .store(in: &cancellables)
    }
    
    private func requestPermissions() {
        // í”„ë¦¬ë·°/ìŠ¤ëƒ…ìƒ·ì—ì„œëŠ” ë°”ë¡œ authorized ì²˜ë¦¬
        if ProcessInfo.processInfo.environment["XCODE_RUNNING_FOR_PREVIEWS"] == "1" {
            self.isAuthorized = true
            return
        }
        recognizer.requestAuthorization { [weak self] ok in
            self?.isAuthorized = ok
        }
    }


    // MARK: - Public API (Viewì—ì„œ í˜¸ì¶œ)

    /// ë…¹ìŒ ì‹œì‘/ì¤‘ì§€
    func toggleRecording() {
        if isListening {
            stopRecording()
        } else {
            startListening()
        }
    }

    /// ë…¹ìŒ ì‹œì‘
    func startListening() {
        // í˜¹ì‹œ TTS ì¤‘ì´ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if synthesizer.isSpeaking { 
            synthesizer.stop() 
        }
        
        voiceState = .userSpeaking

        do {
            try recognizer.startRecording()
        } catch {
            voiceState = .idle
            print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:", error.localizedDescription)
            // í”„ë¦¬ë·° ëª¨ë“œì—ì„œëŠ” ì—ëŸ¬ë¥¼ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
        }
    }

    /// AIì— ë©”ì‹œì§€ ì „ì†¡
    func sendToLLM() async {
        guard !isProcessing else { return }
        isProcessing = true
        defer { isProcessing = false }

        // ì „ì†¡ ì‹œ ë…¹ìŒ ì¤‘ì§€ ë° ì´ˆê¸°í™”
        if isListening {
            stopRecording()
        }

        let input = transcribedText.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !input.isEmpty else {
            print("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        }

        voiceState = .aiSpeaking
        do {
            let entity = try await sendLLMMessageUseCase.execute(requestText: input)
            await handle(entity: entity)
        } catch {
            print("LLM ì‘ë‹µ ì‹¤íŒ¨:", error.localizedDescription)
            voiceState = .idle
        }
    }

    /// ëª©ì—…ìœ¼ë¡œ ì‘ë‹µ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
    func sendToLLMWithMock() async {
        guard !isProcessing else { return }
        isProcessing = true
        defer { isProcessing = false }

        // ì „ì†¡ ì‹œ ë…¹ìŒ ì¤‘ì§€ ë° ì´ˆê¸°í™”
        if isListening {
            stopRecording()
        }

        let input = transcribedText.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !input.isEmpty else {
            print("ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        }

        voiceState = .aiSpeaking
        
        // ëª©ì—… ì‘ë‹µ ìƒì„±
        let mockResponse = LLMMessageEntity(
            requestText: input,
            responseText: "ëª©ì—… ì‘ë‹µ: '\(input)'ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì‘ë‹µì…ë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ AI ì‘ë‹µì´ ì•„ë‹Œ í…ŒìŠ¤íŠ¸ìš© ëª©ì—… ë°ì´í„°ì…ë‹ˆë‹¤.",
            route: nil
        )
        
        await handle(entity: mockResponse)
    }

    /// ë…¹ìŒ ì¤‘ì§€
    func stopRecording() {
        recognizer.stopRecording()
        voiceState = .idle
    }

    /// í•©ì„± ì¤‘ë‹¨
    func stopSpeaking() {
        synthesizer.stop()
        voiceState = .idle
    }

    /// ì·¨ì†Œ (ë…¹ìŒ/í•©ì„± ì¢…ë£Œ + í…ìŠ¤íŠ¸ ì´ˆê¸°í™”)
    func cancel() {
        recognizer.stopRecording()
        synthesizer.stop()
        transcribedText = ""
        llmResponse = ""
        voiceState = .idle
        isProcessing = false
    }

    /// ì „ì†¡ ì™„ë£Œ í›„ ì´ˆê¸°í™”
    func resetAfterResponse() {
        transcribedText = ""
        voiceState = .idle
    }

    // MARK: - Private Handlers

    /// ì„±ê³µ ì²˜ë¦¬: í…ìŠ¤íŠ¸/í•©ì„±/ë¼ìš°íŒ…
    private func handle(entity: LLMMessageEntity) async {
        // UI ì—…ë°ì´íŠ¸
        llmResponse = entity.responseText
        
        // ìŒì„± í•©ì„±
        synthesizer.speak(text: entity.responseText)

        // ë¼ìš°íŒ… í•„ìš” ì‹œ ì•Œë¦¼
        if let domain = entity.route,
           let app = RouteAdapter.map(domain) {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.15) { [weak self] in
                self?.onRoute?(app)
            }
        }
        
        // ìŒì„± í•©ì„± ì™„ë£Œ í›„ ì´ˆê¸°í™”
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) { [weak self] in
            self?.resetAfterResponse()
        }
    }
}
