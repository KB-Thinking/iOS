//
//  VoiceConversationViewModel.swift
//  Kb-Thinking
//
//  Created by jeonguk29 on 8/8/25.
//

import Foundation
import SwiftUI
import Combine

@MainActor
final class VoiceConversationViewModel: ObservableObject {
    
    // MARK: - Published Properties
    
    @Published var transcribedText: String = ""
    @Published var llmResponse = ""
    @Published var voiceState: VoiceState = .idle
    var onRoute: ((AppRoute?) -> Void)?
    
    // MARK: - Dependencies
    
    private let recognizer = SpeechRecognizerManager()
    private let synthesizer = SpeechSynthesizerManager()
    private let sendLLMMessageUseCase: SendLLMMessageUseCase
    
    private var cancellables = Set<AnyCancellable>()
    
    init(sendLLMMessageUseCase: SendLLMMessageUseCase) {
        self.sendLLMMessageUseCase = sendLLMMessageUseCase
        
        recognizer.requestAuthorization { status in
            print("ê¶Œí•œ ìƒíƒœ: \(status)")
        }

        // Combine ë°”ì¸ë”©
        recognizer.$recognizedText
            .receive(on: DispatchQueue.main)
            .assign(to: &$transcribedText)
    }
    
    // MARK: - Methods
    
    func startListening() {
        voiceState = .userSpeaking
        transcribedText = ""

        do {
            try recognizer.startRecording()
        } catch {
            print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:", error.localizedDescription)
        }
    }
    
    func stopListeningAndAskLLM() async {
        recognizer.stopRecording()
        let input = transcribedText
        
        do {
            voiceState = .aiSpeaking
            let entity = try await sendLLMMessageUseCase.execute(requestText: input)
            transcribedText = entity.responseText
            synthesizer.speak(text: entity.responseText)
        } catch {
            transcribedText = "âŒ ì‘ë‹µ ì‹¤íŒ¨: \(error.localizedDescription)"
        }
        
        voiceState = .idle
    }
    
    func stopSpeaking() {
        synthesizer.stop()
        voiceState = .idle
    }
    
    // MARK: - ëª©ì—… ì‘ë‹µ
    func stopListeningAndRespondWithMock() async {
        recognizer.stopRecording()
        voiceState = .aiSpeaking
        
        // 4ë²ˆì§¸ ëª©ì—… ê³ ì •
        let mock = LLMMessageEntity.mockList[3]
        
        transcribedText = mock.responseText
        synthesizer.speak(text: mock.responseText)
        
        // routeê°€ ìˆìœ¼ë©´ í™”ë©´ ì´ë™ ì½œë°± ì‹¤í–‰
        if let domainRoute = mock.route,
           let appRoute = RouteAdapter.map(domainRoute) {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.15) {
                self.onRoute?(appRoute)
            }
        }
        
        voiceState = .idle
    }
}

// MARK: - ë¼ìš°í„° ê´€ë ¨
extension VoiceConversationViewModel {
    func send(_ text: String) {
        Task {
            do {
                let entity = try await sendLLMMessageUseCase.execute(requestText: text)
                await MainActor.run {
                    self.llmResponse = entity.responseText
                    if let domainRoute = entity.route,
                       let appRoute = RouteAdapter.map(domainRoute) {
                        self.onRoute?(appRoute)
                    }
                }
            } catch {
                // ì—ëŸ¬ ì²˜ë¦¬ (í† ìŠ¤íŠ¸/ì•ŒëŸ¿ ë“±)
            }
        }
    }
}
